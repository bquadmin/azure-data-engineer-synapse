{
	"name": "nb_consumerindex_raw",
	"properties": {
		"folder": {
			"name": "best_of_class_recruiting"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "dlpssp01",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": true,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "cd5f4654-fa2e-43d9-9fac-26ed631cd6d3"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/b99affbe-2256-409d-a682-c20a3963070b/resourceGroups/dlparg4dev05/providers/Microsoft.Synapse/workspaces/dlpsws4dev04/bigDataPools/dlpssp01",
				"name": "dlpssp01",
				"type": "Spark",
				"endpoint": "https://dlpsws4dev04.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/dlpssp01",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.2",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 15
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Ingest CU data to raw\r\n",
					"The latsest consumer price index is provided publicly via the [Bureau of Labor Statistics website](https://www.bls.gov/cpi/data.htm). This script is reload raw tables with the latest files from a blob storage location. Currently there is no check on if the data has changed at the source.\r\n",
					"\r\n",
					"\r\n",
					"## Steps\r\n",
					"1. Create raw_cu database (if does not exist) \r\n",
					"2. Read all tables from source (external) ADLS path.\r\n",
					"3. Save as parquet tables in raw \r\n",
					"4. Log timing for each step"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%%configure -f\r\n",
					"{\r\n",
					"    \"driverMemory\": \"28g\",\r\n",
					"    \"driverCores\": 4,\r\n",
					"    \"executorMemory\": \"28g\",\r\n",
					"    \"executorCores\": 4,\r\n",
					"    \"conf\":\r\n",
					"    {\r\n",
					"        \"spark.driver.maxResultSize\": \"10g\",\r\n",
					"        \"spark.sql.execution.arrow.pyspark.enabled\": \"false\",\r\n",
					"        \"spark.sql.shuffle.partitions\": 24  \r\n",
					"    }\r\n",
					"}"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run utils/logging_utils"
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"keyvault_ls = \"dlpkvs_ls\""
				],
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": []
				},
				"source": [
					"source_storage_base_path = mssparkutils.credentials.getSecretWithLS(keyvault_ls, 'cu-source-path')\r\n",
					"raw_storage_base_path = mssparkutils.credentials.getSecretWithLS(keyvault_ls, 'raw-datalake-path')\r\n",
					"refined_storage_base_path =  mssparkutils.credentials.getSecretWithLS(keyvault_ls, 'refined-datalake-path')\r\n",
					"curated_storage_base_path =  mssparkutils.credentials.getSecretWithLS(keyvault_ls, 'curated-datalake-path')\r\n",
					"\r\n",
					"raw_format = \"parquet\""
				],
				"execution_count": 10
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"job_name = \"consumerindex_ingest\"\r\n",
					"destination_database_name = \"raw\"\r\n",
					"destination_table_name = \"\"\r\n",
					"source_table_name = \"consumerindex_parquet_files\"\r\n",
					"start_logging(job_name, destination_database_name, destination_table_name, source_table_name)"
				],
				"execution_count": 11
			},
			{
				"cell_type": "code",
				"source": [
					"def create_database(db_name, path, drop=False):\r\n",
					"    if drop:\r\n",
					"        spark.sql(f\"DROP DATABASE IF EXISTS {db_name} CASCADE;\")    \r\n",
					"    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {db_name} LOCATION '{path}'\")\r\n",
					"\r\n",
					"\r\n",
					"def get_table_paths(base_path):\r\n",
					"    \"\"\"Return list of items for file path and folder/table.\r\n",
					"\r\n",
					"    Args:\r\n",
					"        base_path (str): Azure path to get data directories (must be authenticated), \r\n",
					"            formatted abfss://<container>@<storage_account>.dfs.core.windows.net/[root_directory]\r\n",
					"\r\n",
					"    Returns:\r\n",
					"        list[(path, table_name)]: List of tuples, each item has full path in ADLS and the folder name (usually used as table name)\r\n",
					"    \r\n",
					"    \"\"\"\r\n",
					"    table_paths = mssparkutils.fs.ls(base_path)\r\n",
					"    return [(file_info.path, file_info.name) for file_info in table_paths]\r\n",
					"\r\n",
					"\r\n",
					"def save_spark_table(df, table, partition_str=None, format=\"parquet\", mode='overwrite'):\r\n",
					"    \"\"\"Save Spark table using format provided (parquet or delta).\r\n",
					"    \r\n",
					"    Args:\r\n",
					"        df (DataFrame): Data to save as Spark table.\r\n",
					"        table (str): Name of destination table\r\n",
					"        partition_str (str, optional): Column names used to partition folders as comma delimited string, for example 'colA,colB'. No partitions created if not provided.\r\n",
					"        format (str, optional): Defaults to 'delta'\r\n",
					"\r\n",
					"    Returns:\r\n",
					"        None\r\n",
					"\r\n",
					"    \"\"\"\r\n",
					"    if partition_str:\r\n",
					"        df.write.mode(\"overwrite\").format(format).partitionBy(partition_str).saveAsTable(table)\r\n",
					"    else:\r\n",
					"        df.write.mode(\"overwrite\").format(format).saveAsTable(table)\r\n",
					"    log_informational_message(f\"Saved raw table {table}\")"
				],
				"execution_count": 12
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"source_base_path = source_storage_base_path + 'cu_parquet'\r\n",
					"raw_path = raw_storage_base_path + 'cu'\r\n",
					"raw_db = 'raw_cu'\r\n",
					"\r\n",
					"create_database(raw_db, raw_path, drop=True)\r\n",
					"\r\n",
					"table_list = get_table_paths(source_base_path)\r\n",
					"\r\n",
					"print(table_list)"
				],
				"execution_count": 13
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"for path, destination_table in table_list:\r\n",
					"    source_df = spark.read.parquet(path)\r\n",
					"   \r\n",
					"    # FOR PROFILING AND EXPLORATION\r\n",
					"    print(destination_table)\r\n",
					"    # source_df.show()\r\n",
					"\r\n",
					"    save_spark_table(source_df, raw_db+'.'+destination_table, format=raw_format)    "
				],
				"execution_count": 15
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"stop_logging(job_name)"
				],
				"execution_count": 16
			}
		]
	}
}