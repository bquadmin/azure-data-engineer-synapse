{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "dlpsws4dev04"
		},
		"dlpsws4dev04-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'dlpsws4dev04-WorkspaceDefaultSqlServer'"
		},
		"dlpkvs_ls_properties_typeProperties_baseUrl": {
			"type": "string",
			"defaultValue": "https://dlpkvs4dev03.vault.azure.net/"
		},
		"dlpsws4dev04-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://dlpssa4dev04.dfs.core.windows.net/"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/dlpkvs_ls')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureKeyVault",
				"typeProperties": {
					"baseUrl": "[parameters('dlpkvs_ls_properties_typeProperties_baseUrl')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dlpsws4dev04-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('dlpsws4dev04-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dlpsws4dev04-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('dlpsws4dev04-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				},
				"managedVirtualNetwork": {
					"type": "ManagedVirtualNetworkReference",
					"referenceName": "default"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/managedVirtualNetworks/default')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sql_permissions')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- This is an example of setting permissions on serverless SQL database\nCREATE DATABASE severless_db\n\nuse master\ngo\n\nCREATE LOGIN [test@trainingdustinvannoy.onmicrosoft.com] FROM EXTERNAL PROVIDER;\ngo\nCREATE LOGIN [synapse_reader] FROM EXTERNAL PROVIDER;\ngo\n\nALTER SERVER ROLE sysadmin ADD MEMBER [synapse_reader];\n--ALTER SERVER ROLE public ADD MEMBER [synapse_reader];\n\nalter role db_datareader add MEMBER [synapse_reader]\nGO\n\nalter role db_datareader add MEMBER [synapse_reader]\nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/logging_utils')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "utils"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "ed7f9576-1de0-491f-9178-cc1d366c8198"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"# Parameters:\r\n",
							"# Set test_run to True only when running tests. Keep it as False (default) for normal use\r\n",
							"test_run= True #False"
						],
						"outputs": [],
						"execution_count": 103
					},
					{
						"cell_type": "code",
						"source": [
							"from dataclasses import dataclass, asdict\r\n",
							"import time\r\n",
							"\r\n",
							"@dataclass\r\n",
							"class LogMessage():\r\n",
							"    message: str\r\n",
							"    job_name: str\r\n",
							"    destination_database_name: str = \"\"\r\n",
							"    destination_table_name: str = \"\"\r\n",
							"    source_table_name: str =\"\"\r\n",
							"    duration: str = \"0\"\r\n",
							"    job_start_time: str = \"\"\r\n",
							"    job_end_time: str = \"\"\r\n",
							"\r\n",
							"base_message = None\r\n",
							"\r\n",
							"def print_message(start=False, stop=False):\r\n",
							"    msg_str = '{\"message\": \"' + base_message.message + '\", \"job_name\": \"' + base_message.job_name + '\"'\r\n",
							"    if start or stop:\r\n",
							"        msg_str += ', \"destination\": \"' + base_message.destination_database_name + '.' + base_message.destination_table_name + '\"'\r\n",
							"    if stop:\r\n",
							"         msg_str += ', \"duration\": \"' + base_message.duration + '\"'\r\n",
							"    msg_str += '}'\r\n",
							"    return msg_str\r\n",
							"\r\n",
							"spark_log4j = sc._jvm.org.apache.log4j\r\n",
							"logger = spark_log4j.LogManager.getLogger(\"synapse_dataplatform\")"
						],
						"outputs": [],
						"execution_count": 112
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def start_logging(job_name, destination_database_name, destination_table_name, source_table_name):\r\n",
							"    job_start_time = time.perf_counter()\r\n",
							"    \r\n",
							"    global base_message\r\n",
							"    base_message = LogMessage(message=\"\", job_name=job_name, destination_database_name=destination_database_name, destination_table_name=destination_table_name, job_start_time=job_start_time)\r\n",
							"\r\n",
							"    base_message.message=f\"Starting logger for {job_name}\"\r\n",
							"    logger.info(print_message(start=True))"
						],
						"outputs": [],
						"execution_count": 105
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def stop_logging(job_name):\r\n",
							"    base_message.job_end_time = time.perf_counter()\r\n",
							"    base_message.duration = str(base_message.job_end_time - base_message.job_start_time)\r\n",
							"    base_message.message = f\"Stopping logger for {job_name}\"\r\n",
							"    logger.info(print_message(stop=True))"
						],
						"outputs": [],
						"execution_count": 106
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def log_informational_message(message):\r\n",
							"    base_message.message = message\r\n",
							"    logger.info(print_message())"
						],
						"outputs": [],
						"execution_count": 107
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def log_warning_message(message):\r\n",
							"    base_message.message = message\r\n",
							"    logger.warn(print_message())"
						],
						"outputs": [],
						"execution_count": 108
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def log_error_message(message):\r\n",
							"    base_message.message = message\r\n",
							"    logger.error(print_message())"
						],
						"outputs": [],
						"execution_count": 109
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"if test_run:\r\n",
							"    start_logging('log_test_job', 'log_test_destination_db', 'log_test_destination_table', 'log_test_source_table')\r\n",
							"    log_informational_message('Log test informational message')\r\n",
							"    stop_logging('log_test_job')"
						],
						"outputs": [],
						"execution_count": 113
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/logging_utils_json')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "utils"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "2499df63-f6b6-4a55-843b-63e98d680cf4"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from dataclasses import dataclass\r\n",
							"from dataclasses_json import dataclass_json\r\n",
							"import time\r\n",
							"\r\n",
							"@dataclass_json\r\n",
							"@dataclass\r\n",
							"class LogMessage():\r\n",
							"    message: str\r\n",
							"    job_name: str\r\n",
							"    destination_database_name: str = \"\"\r\n",
							"    destination_table_name: str = \"\"\r\n",
							"    source_table_name: str =\"\"\r\n",
							"    duration: str = \"0\"\r\n",
							"    job_start_time: str = \"\"\r\n",
							"    job_end_time: str = \"\"\r\n",
							"\r\n",
							"base_message = None\r\n",
							"spark_log4j = sc._jvm.org.apache.log4j\r\n",
							"logger = spark_log4j.LogManager.getLogger(\"synapse_dataplatform\")"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def start_logging(job_name, destination_database_name, destination_table_name, source_table_name):\r\n",
							"    job_start_time = time.perf_counter()\r\n",
							"    \r\n",
							"    global base_message\r\n",
							"    base_message = LogMessage(message=\"\", job_name=job_name, destination_database_name=destination_database_name, destination_table_name=destinatoin_table_name)\r\n",
							"\r\n",
							"    base_message.message=f\"Starting logger for {job_name}\"\r\n",
							"    logger.info(base_message.to_json())"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def stop_logging(job_name):\r\n",
							"    base_message.job_end_time = time.perf_counter()\r\n",
							"    base_message.duration = base_message.job_end_time - base_message.job_start_time\r\n",
							"    base_message.message = f\"Stopping logger for {job_name}\"\r\n",
							"    logger.info(base_message.to_json())"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def log_informational_message(message):\r\n",
							"    base_message.message = message\r\n",
							"    logger.info(base_message.to_json())"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def log_warning_message(message):\r\n",
							"    base_message.message = message\r\n",
							"    logger.warn(base_message.to_json())"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def log_error_message(message):\r\n",
							"    base_message.message = message\r\n",
							"    logger.error(base_message.to_json())"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/nb_consumerindex_raw')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "best_of_class_recruiting"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "dlpssp4dev04",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "6796e418-a844-4e00-84a4-d8d20cbf5ad7"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/b99affbe-2256-409d-a682-c20a3963070b/resourceGroups/dlparg4dev05/providers/Microsoft.Synapse/workspaces/dlpsws4dev04/bigDataPools/dlpssp4dev04",
						"name": "dlpssp4dev04",
						"type": "Spark",
						"endpoint": "https://dlpsws4dev04.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/dlpssp4dev04",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 5,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 15
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Ingest CU data to raw\r\n",
							"The latsest consumer price index is provided publicly via the [Bureau of Labor Statistics website](https://www.bls.gov/cpi/data.htm). This script is to download and reload raw tables with the latest files. Currently there is no check on if the data has changed at the source.\r\n",
							"\r\n",
							"\r\n",
							"## Steps\r\n",
							"1. Create raw_cu database (if does not exist) \r\n",
							"2. Download all \"enabled\" files from website\r\n",
							"3. Save as parquet tables in raw \r\n",
							"4. Log timing for each step"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%%configure -f\r\n",
							"{\r\n",
							"    \"driverMemory\": \"28g\",\r\n",
							"    \"driverCores\": 4,\r\n",
							"    \"executorMemory\": \"28g\",\r\n",
							"    \"executorCores\": 4,\r\n",
							"    \"conf\":\r\n",
							"    {\r\n",
							"        \"spark.driver.maxResultSize\": \"10g\",\r\n",
							"        \"spark.sql.execution.arrow.pyspark.enabled\": \"false\",\r\n",
							"        \"spark.sql.shuffle.partitions\": 24  \r\n",
							"    }\r\n",
							"}"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"keyvault_ls = \"dlpkvs_ls\""
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": []
						},
						"source": [
							"\r\n",
							"raw_storage_base_path = mssparkutils.credentials.getSecretWithLS(keyvault_ls, 'raw-datalake-path')\r\n",
							"refined_storage_base_path =  mssparkutils.credentials.getSecretWithLS(keyvault_ls, 'refined-datalake-path')\r\n",
							"curated_storage_base_path =  mssparkutils.credentials.getSecretWithLS(keyvault_ls, 'curated-datalake-path')"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"source": [
							"import pandas as pd\r\n",
							"import requests\r\n",
							"import io\r\n",
							"\r\n",
							"\r\n",
							"def create_database(db_name, path, drop=False):\r\n",
							"    if drop:\r\n",
							"        spark.sql(f\"DROP DATABASE IF EXISTS {db_name} CASCADE;\")    \r\n",
							"    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {db_name} LOCATION '{path}';\")\r\n",
							"\r\n",
							"\r\n",
							"def read_csv_from_web(url):\r\n",
							"    \"\"\"Read delimited data from web, for example url = https://download.bls.gov/pub/time.series/cu/cu.area\r\n",
							"    \r\n",
							"    Args:\r\n",
							"        url: URL to raw file\r\n",
							"\r\n",
							"    Returns:\r\n",
							"        DataFrame, results of reading data with extra spaces stripped from column headers\r\n",
							"    \"\"\"\r\n",
							"    s = requests.get(url).content\r\n",
							"    c = pd.read_csv(url, delimiter='\\t', na_filter=False)\r\n",
							"\r\n",
							"    col_headers = []\r\n",
							"    for h in c.columns:\r\n",
							"        col_headers.append(h.strip())\r\n",
							"\r\n",
							"    return spark.createDataFrame(c, col_headers)\r\n",
							"\r\n",
							"\r\n",
							"def get_table_paths(base_path):\r\n",
							"    \"\"\"Return list of items for file path and folder/table.\r\n",
							"\r\n",
							"    Args:\r\n",
							"        base_path (str): Azure path to get data directories (must be authenticated), \r\n",
							"            formatted abfss://<container>@<storage_account>.dfs.core.windows.net/[root_directory]\r\n",
							"\r\n",
							"    Returns:\r\n",
							"        list[(path, table_name)]: List of tuples, each item has full path in ADLS and the folder name (usually used as table name)\r\n",
							"    \r\n",
							"    \"\"\"\r\n",
							"    table_paths = mssparkutils.fs.ls(base_path)\r\n",
							"    return [(file_info.path, file_info.name) for file_info in table_paths]\r\n",
							"\r\n",
							"\r\n",
							"def save_table(df, table, partition_str=None, format=\"delta\", mode='overwrite'):\r\n",
							"    \"\"\"Save Spark table using format provided (parquet or delta).\r\n",
							"    \r\n",
							"    Args:\r\n",
							"        df (DataFrame): Data to save as Spark table.\r\n",
							"        table (str): Name of destination table\r\n",
							"        partition_str (str, optional): Column names used to partition folders as comma delimited string, for example 'colA,colB'. No partitions created if not provided.\r\n",
							"        format (str, optional): Defaults to 'delta'\r\n",
							"\r\n",
							"    Returns:\r\n",
							"        None\r\n",
							"\r\n",
							"    \"\"\"\r\n",
							"    options = {\r\n",
							"        \"mergeSchema\": \"true\"\r\n",
							"    }\r\n",
							"    writer = df.write.mode(\"overwrite\").options(**options).format(format)\r\n",
							"    if partition_str:\r\n",
							"        writer = writer.partitionBy(partition_str)\r\n",
							"    writer.saveAsTable(table)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"raw_path = raw_storage_base_path + 'cu'\r\n",
							"\r\n",
							"create_database('raw_cu', raw_path)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"file_map = {\r\n",
							"    'cu.series': 'series',\r\n",
							"    'cu.data.0.Current': 'current',\r\n",
							"    'cu.area': 'area',\r\n",
							"    'cu.base': 'base',\r\n",
							"    'cu.item': 'item',\r\n",
							"    'cu.period': 'period',    \r\n",
							"    'cu.periodicity': 'periodicity',\r\n",
							"    'cu.seasonal': 'seasonal',\r\n",
							"\r\n",
							"    # 'cu.data.1.AllItems': 'all_items',\r\n",
							"    # 'cu.data.2.Summaries': 'summaries',\r\n",
							"    # 'cu.data.3.AsizeNorthEast': 'asize_north_east',\r\n",
							"    # 'cu.data.4.AsizeNorthCentral': 'asize_north_central',\r\n",
							"    # 'cu.data.5.AsizeSouth': 'asize_south',\r\n",
							"    # 'cu.data.6.AsizeWest': 'asize_west',\r\n",
							"    # 'cu.data.7.OtherNorthEast': 'other_north_east',\r\n",
							"    # 'cu.data.8.OtherNorthCentral': 'other_north_central',\r\n",
							"    # 'cu.data.9.OtherSouth': 'other_south',\r\n",
							"    # 'cu.data.10.OtherWest': 'other_west',\r\n",
							"    # 'cu.data.11.USFoodBeverage': 'us_food_beverage',\r\n",
							"    # 'cu.data.12.USHousing': 'us_housing',\r\n",
							"    # 'cu.data.13.USApparel': 'us_apparel',\r\n",
							"    # 'cu.data.14.USTransportation': 'us_transportation',\r\n",
							"    # 'cu.data.15.USMedical': 'us_medical',\r\n",
							"    # 'cu.data.16.USRecreation': 'us_recreation',\r\n",
							"    # 'cu.data.17.USEducationAndCommunication': 'us_educational_and_communication',\r\n",
							"    # 'cu.data.18.USOtherGoodsAndServices': 'us_other_goods_and_services',\r\n",
							"    # 'cu.data.19.PopulationSize': 'population_size',\r\n",
							"    # 'cu.data.20.USCommoditiesServicesSpecial': 'us_commodities_services_special'\r\n",
							"    \r\n",
							"}"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# url = 'https://download.bls.gov/pub/time.series/cu/cu.data.0.Current'\r\n",
							"# destination_table = 'current'\r\n",
							"\r\n",
							"# source_df = read_csv_from_web(url)\r\n",
							"# display(source_df)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"for file, destination_table in file_map.items():\r\n",
							"    url = f'https://download.bls.gov/pub/time.series/cu/{file}'\r\n",
							"    source_df = read_csv_from_web(url)\r\n",
							"    \r\n",
							"    # FOR PROFILING AND EXPLORATION\r\n",
							"    print(destination_table)\r\n",
							"    # source_df.show()\r\n",
							"\r\n",
							"    save_spark_table(source_df,'raw_cu.'+destination_table)    "
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE VIEW raw_cu.vw_current_full\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    c.year,\r\n",
							"    c.period,\r\n",
							"    s.*, \r\n",
							"    item.item_name,\r\n",
							"    area.area_name,\r\n",
							"    c.value\r\n",
							"FROM `current` c\r\n",
							"  JOIN series s\r\n",
							"    ON c.series_id = s.series_id\r\n",
							"  JOIN item\r\n",
							"    ON s.item_code = item.item_code\r\n",
							"  JOIN area\r\n",
							"    ON s.area_code = area.area_code"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"SELECT *\r\n",
							"FROM raw_cu.vw_current_full\r\n",
							"LIMIT 10"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# from pyspark.sql.functions import input_file_name\r\n",
							"# current_df = spark.read.format('delta').load(raw_path).withColumn('path', input_file_name())\r\n",
							"# display(current_df)"
						],
						"outputs": [],
						"execution_count": 11
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/nb_jobposts_raw')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "best_of_class_recruiting"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "dlpssp4dev04",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "ea4b618f-be7f-4410-bf0f-c5da716e7d55"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/b99affbe-2256-409d-a682-c20a3963070b/resourceGroups/dlparg4dev05/providers/Microsoft.Synapse/workspaces/dlpsws4dev04/bigDataPools/dlpssp4dev04",
						"name": "dlpssp4dev04",
						"type": "Spark",
						"endpoint": "https://dlpsws4dev04.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/dlpssp4dev04",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 5,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 15
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Raw streaming of Job Posts\r\n",
							"Do this one with Databricks."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/nb_refined')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "best_of_class_recruiting"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "dlpssp4dev04",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "bb8b99a6-1b16-4b5f-a9a6-6fbb4210804a"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/b99affbe-2256-409d-a682-c20a3963070b/resourceGroups/dlparg4dev05/providers/Microsoft.Synapse/workspaces/dlpsws4dev04/bigDataPools/dlpssp4dev04",
						"name": "dlpssp4dev04",
						"type": "Spark",
						"endpoint": "https://dlpsws4dev04.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/dlpssp4dev04",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 5,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 15
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Delta refined and curated\r\n",
							"sql = 'CREATE DATABASE IF NOT EXISTS refined'"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dlpssp4dev04')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 20,
					"minNodeCount": 3
				},
				"nodeCount": 5,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "2.4",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus2"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dlpsdp4dev04')]",
			"type": "Microsoft.Synapse/workspaces/sqlPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"collation": "SQL_Latin1_General_CP1_CI_AS",
				"maxSizeBytes": 263882790666240,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus2"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/default')]",
			"type": "Microsoft.Synapse/workspaces/managedVirtualNetworks",
			"apiVersion": "2019-06-01-preview",
			"properties": {},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/default/dlpkvs-pe')]",
			"type": "Microsoft.Synapse/workspaces/managedVirtualNetworks/managedPrivateEndpoints",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"privateLinkResourceId": "/subscriptions/b99affbe-2256-409d-a682-c20a3963070b/resourceGroups/dlparg4dev01/providers/Microsoft.KeyVault/vaults/dlpkvs4dev03",
				"groupId": "vault"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/managedVirtualNetworks/default')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/default/synapse-ws-sql--dlpsws4dev04')]",
			"type": "Microsoft.Synapse/workspaces/managedVirtualNetworks/managedPrivateEndpoints",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"privateLinkResourceId": "/subscriptions/b99affbe-2256-409d-a682-c20a3963070b/resourceGroups/dlparg4dev05/providers/Microsoft.Synapse/workspaces/dlpsws4dev04",
				"groupId": "sql",
				"fqdns": [
					"dlpsws4dev04.2ab8c5b9-68f1-4162-8b3a-a38df77c71e4.sql.azuresynapse.net"
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/managedVirtualNetworks/default')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/default/synapse-ws-sqlOnDemand--dlpsws4dev04')]",
			"type": "Microsoft.Synapse/workspaces/managedVirtualNetworks/managedPrivateEndpoints",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"privateLinkResourceId": "/subscriptions/b99affbe-2256-409d-a682-c20a3963070b/resourceGroups/dlparg4dev05/providers/Microsoft.Synapse/workspaces/dlpsws4dev04",
				"groupId": "sqlOnDemand",
				"fqdns": [
					"dlpsws4dev04-ondemand.2ab8c5b9-68f1-4162-8b3a-a38df77c71e4.sql.azuresynapse.net"
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/managedVirtualNetworks/default')]"
			]
		}
	]
}